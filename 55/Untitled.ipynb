{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e5cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Импортируем необходимые библиотеки для создания генеративно-состязательной сети\n",
    "Код разработан в основном с использованием библиотеки PyTorch\n",
    "\"\"\"\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "#from model import discriminator, generator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae146f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Определяем, доступны ли какие-либо графические процессоры\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f93a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nСетевые архитектуры\\nНиже приведены архитектуры дискриминатора и генератора\\n\\nclass discriminator(nn.Module):\\n    def __init__(self):\\n        super(discriminator, self).__init__()\\n        self.fc1 = nn.Linear(784, 512)\\n        self.fc2 = nn.Linear(512, 1)\\n        self.activation = nn.LeakyReLU(0.1)\\n    def forward(self, x):\\n        x = x.view(-1, 784)\\n        x = self.activation(self.fc1(x))\\n        x = self.fc2(x)\\n        return nn.Sigmoid()(x)\\nclass generator(nn.Module):\\n    def __init__(self):\\n        super(generator, self).__init__()\\n        self.fc1 = nn.Linear(128, 1024)\\n        self.fc2 = nn.Linear(1024, 2048)\\n        self.fc3 = nn.Linear(2048, 784)\\n        self.activation = nn.ReLU()\\n    def forward(self, x):\\n        x = self.activation(self.fc1(x))\\n        x = self.activation(self.fc2(x))\\n        x = self.fc3(x)\\n        x = x.view(-1, 1, 28, 28)\\n        return nn.Tanh()(x)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Сетевые архитектуры\n",
    "Ниже приведены архитектуры дискриминатора и генератора\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return nn.Sigmoid()(x)\n",
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 784)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return nn.Tanh()(x)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dd1e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "490d34ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m D \u001b[38;5;241m=\u001b[39m discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m G_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(G\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.999\u001b[39m))\n",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m, in \u001b[0;36mgenerator.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28msuper\u001b[39m(\u001b[43mGenerator\u001b[49m, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m      7\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(z_dim, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         nn\u001b[38;5;241m.\u001b[39mTanh()\n\u001b[1;32m     13\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Determine if any GPUs are available\n",
    "\"\"\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Hyperparameter settings\n",
    "\"\"\"\n",
    "epochs = 150\n",
    "lr = 2e-4\n",
    "batch_size = 64\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# Model\n",
    "G = generator().to(device)\n",
    "D = discriminator().to(device)\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Image transformation and dataloader creation\n",
    "Note that we are training generation and not classification, and hence\n",
    "only the train_loader is loaded\n",
    "\"\"\"\n",
    "# Transform\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Load data\n",
    "train_set = datasets.MNIST('mnist/', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Network training procedure\n",
    "Every step both the loss for disciminator and generator is updated\n",
    "Discriminator aims to classify reals and fakes\n",
    "Generator aims to generate images as realistic as possible\n",
    "\"\"\"\n",
    "\"\"\"for epoch in range(epochs):\n",
    "    for idx, (imgs, _) in enumerate(train_loader):\n",
    "        idx += 1\n",
    "\n",
    "        # Training the discriminator\n",
    "        # Real inputs are actual images of the MNIST dataset\n",
    "        # Fake inputs are from the generator\n",
    "        # Real inputs should be classified as 1 and fake as 0\n",
    "        real_inputs = imgs.to(device)\n",
    "        real_outputs = D(real_inputs)\n",
    "        real_label = torch.ones(real_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_label = torch.zeros(fake_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_label), 0)\n",
    "\n",
    "        D_loss = loss(outputs, targets)\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Training the generator\n",
    "        # For generator, goal is to make the discriminator believe everything is 1\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128)-0.5)/0.5\n",
    "        noise = noise.to(device)\n",
    "\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_targets = torch.ones([fake_inputs.shape[0], 1]).to(device)\n",
    "        G_loss = loss(fake_outputs, fake_targets)\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0 or idx == len(train_loader):\n",
    "            print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'.format(epoch, idx, D_loss.item(), G_loss.item()))\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(G, 'Generator_epoch_{}.pth'.format(epoch))\n",
    "        print('Model saved.')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3a7e55e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 100: discriminator_loss 0.701 generator_loss 0.869\n",
      "Epoch 0 Iteration 200: discriminator_loss 0.655 generator_loss 0.844\n",
      "Epoch 0 Iteration 300: discriminator_loss 0.625 generator_loss 0.833\n",
      "Epoch 0 Iteration 400: discriminator_loss 0.676 generator_loss 0.752\n",
      "Epoch 0 Iteration 500: discriminator_loss 0.664 generator_loss 0.638\n",
      "Epoch 0 Iteration 600: discriminator_loss 0.698 generator_loss 0.717\n",
      "Epoch 0 Iteration 700: discriminator_loss 0.653 generator_loss 0.727\n",
      "Epoch 0 Iteration 800: discriminator_loss 0.660 generator_loss 0.726\n",
      "Epoch 0 Iteration 900: discriminator_loss 0.675 generator_loss 0.849\n",
      "Epoch 0 Iteration 938: discriminator_loss 0.652 generator_loss 0.724\n",
      "Epoch 1 Iteration 100: discriminator_loss 0.696 generator_loss 0.729\n",
      "Epoch 1 Iteration 200: discriminator_loss 0.676 generator_loss 0.806\n",
      "Epoch 1 Iteration 300: discriminator_loss 0.674 generator_loss 0.697\n",
      "Epoch 1 Iteration 400: discriminator_loss 0.672 generator_loss 0.716\n",
      "Epoch 1 Iteration 500: discriminator_loss 0.691 generator_loss 0.822\n",
      "Epoch 1 Iteration 600: discriminator_loss 0.696 generator_loss 0.811\n",
      "Epoch 1 Iteration 700: discriminator_loss 0.659 generator_loss 0.670\n",
      "Epoch 1 Iteration 800: discriminator_loss 0.683 generator_loss 0.760\n",
      "Epoch 1 Iteration 900: discriminator_loss 0.674 generator_loss 0.826\n",
      "Epoch 1 Iteration 938: discriminator_loss 0.660 generator_loss 0.681\n",
      "Epoch 2 Iteration 100: discriminator_loss 0.669 generator_loss 0.802\n",
      "Epoch 2 Iteration 200: discriminator_loss 0.656 generator_loss 0.862\n",
      "Epoch 2 Iteration 300: discriminator_loss 0.666 generator_loss 0.623\n",
      "Epoch 2 Iteration 400: discriminator_loss 0.729 generator_loss 0.976\n",
      "Epoch 2 Iteration 500: discriminator_loss 0.687 generator_loss 0.764\n",
      "Epoch 2 Iteration 600: discriminator_loss 0.673 generator_loss 0.654\n",
      "Epoch 2 Iteration 700: discriminator_loss 0.658 generator_loss 0.733\n",
      "Epoch 2 Iteration 800: discriminator_loss 0.728 generator_loss 0.619\n",
      "Epoch 2 Iteration 900: discriminator_loss 0.688 generator_loss 0.786\n",
      "Epoch 2 Iteration 938: discriminator_loss 0.643 generator_loss 0.875\n",
      "Epoch 3 Iteration 100: discriminator_loss 0.692 generator_loss 0.907\n",
      "Epoch 3 Iteration 200: discriminator_loss 0.719 generator_loss 1.052\n",
      "Epoch 3 Iteration 300: discriminator_loss 0.670 generator_loss 0.835\n",
      "Epoch 3 Iteration 400: discriminator_loss 0.685 generator_loss 0.680\n",
      "Epoch 3 Iteration 500: discriminator_loss 0.663 generator_loss 0.822\n",
      "Epoch 3 Iteration 600: discriminator_loss 0.682 generator_loss 0.873\n",
      "Epoch 3 Iteration 700: discriminator_loss 0.691 generator_loss 0.959\n",
      "Epoch 3 Iteration 800: discriminator_loss 0.695 generator_loss 0.732\n",
      "Epoch 3 Iteration 900: discriminator_loss 0.679 generator_loss 0.678\n",
      "Epoch 3 Iteration 938: discriminator_loss 0.688 generator_loss 0.792\n",
      "Epoch 4 Iteration 100: discriminator_loss 0.683 generator_loss 0.662\n",
      "Epoch 4 Iteration 200: discriminator_loss 0.667 generator_loss 0.754\n",
      "Epoch 4 Iteration 300: discriminator_loss 0.639 generator_loss 0.773\n",
      "Epoch 4 Iteration 400: discriminator_loss 0.700 generator_loss 0.682\n",
      "Epoch 4 Iteration 500: discriminator_loss 0.655 generator_loss 0.853\n",
      "Epoch 4 Iteration 600: discriminator_loss 0.664 generator_loss 0.743\n",
      "Epoch 4 Iteration 700: discriminator_loss 0.680 generator_loss 0.837\n",
      "Epoch 4 Iteration 800: discriminator_loss 0.718 generator_loss 0.818\n",
      "Epoch 4 Iteration 900: discriminator_loss 0.678 generator_loss 0.774\n",
      "Epoch 4 Iteration 938: discriminator_loss 0.688 generator_loss 0.771\n",
      "Epoch 5 Iteration 100: discriminator_loss 0.663 generator_loss 0.711\n",
      "Epoch 5 Iteration 200: discriminator_loss 0.689 generator_loss 0.757\n",
      "Epoch 5 Iteration 300: discriminator_loss 0.692 generator_loss 0.786\n",
      "Epoch 5 Iteration 400: discriminator_loss 0.679 generator_loss 0.683\n",
      "Epoch 5 Iteration 500: discriminator_loss 0.697 generator_loss 0.614\n",
      "Epoch 5 Iteration 600: discriminator_loss 0.681 generator_loss 0.729\n",
      "Epoch 5 Iteration 700: discriminator_loss 0.654 generator_loss 0.914\n",
      "Epoch 5 Iteration 800: discriminator_loss 0.694 generator_loss 0.745\n",
      "Epoch 5 Iteration 900: discriminator_loss 0.667 generator_loss 0.848\n",
      "Epoch 5 Iteration 938: discriminator_loss 0.685 generator_loss 0.793\n",
      "Epoch 6 Iteration 100: discriminator_loss 0.676 generator_loss 0.856\n",
      "Epoch 6 Iteration 200: discriminator_loss 0.655 generator_loss 0.668\n",
      "Epoch 6 Iteration 300: discriminator_loss 0.673 generator_loss 0.774\n",
      "Epoch 6 Iteration 400: discriminator_loss 0.681 generator_loss 0.660\n",
      "Epoch 6 Iteration 500: discriminator_loss 0.673 generator_loss 0.822\n",
      "Epoch 6 Iteration 600: discriminator_loss 0.686 generator_loss 0.647\n",
      "Epoch 6 Iteration 700: discriminator_loss 0.653 generator_loss 0.743\n",
      "Epoch 6 Iteration 800: discriminator_loss 0.705 generator_loss 0.623\n",
      "Epoch 6 Iteration 900: discriminator_loss 0.690 generator_loss 0.788\n",
      "Epoch 6 Iteration 938: discriminator_loss 0.666 generator_loss 0.664\n",
      "Epoch 7 Iteration 100: discriminator_loss 0.667 generator_loss 0.900\n",
      "Epoch 7 Iteration 200: discriminator_loss 0.687 generator_loss 0.808\n",
      "Epoch 7 Iteration 300: discriminator_loss 0.721 generator_loss 0.783\n",
      "Epoch 7 Iteration 400: discriminator_loss 0.706 generator_loss 0.649\n",
      "Epoch 7 Iteration 500: discriminator_loss 0.673 generator_loss 0.693\n",
      "Epoch 7 Iteration 600: discriminator_loss 0.681 generator_loss 0.716\n",
      "Epoch 7 Iteration 700: discriminator_loss 0.697 generator_loss 0.729\n",
      "Epoch 7 Iteration 800: discriminator_loss 0.661 generator_loss 0.815\n",
      "Epoch 7 Iteration 900: discriminator_loss 0.720 generator_loss 0.794\n",
      "Epoch 7 Iteration 938: discriminator_loss 0.692 generator_loss 0.658\n",
      "Epoch 8 Iteration 100: discriminator_loss 0.678 generator_loss 0.735\n",
      "Epoch 8 Iteration 200: discriminator_loss 0.676 generator_loss 0.632\n",
      "Epoch 8 Iteration 300: discriminator_loss 0.665 generator_loss 0.845\n",
      "Epoch 8 Iteration 400: discriminator_loss 0.677 generator_loss 0.635\n",
      "Epoch 8 Iteration 500: discriminator_loss 0.701 generator_loss 0.698\n",
      "Epoch 8 Iteration 600: discriminator_loss 0.689 generator_loss 0.720\n",
      "Epoch 8 Iteration 700: discriminator_loss 0.664 generator_loss 0.603\n",
      "Epoch 8 Iteration 800: discriminator_loss 0.665 generator_loss 0.792\n",
      "Epoch 8 Iteration 900: discriminator_loss 0.669 generator_loss 0.761\n",
      "Epoch 8 Iteration 938: discriminator_loss 0.687 generator_loss 0.719\n",
      "Epoch 9 Iteration 100: discriminator_loss 0.676 generator_loss 0.724\n",
      "Epoch 9 Iteration 200: discriminator_loss 0.695 generator_loss 0.698\n",
      "Epoch 9 Iteration 300: discriminator_loss 0.651 generator_loss 0.711\n",
      "Epoch 9 Iteration 400: discriminator_loss 0.716 generator_loss 0.970\n",
      "Epoch 9 Iteration 500: discriminator_loss 0.692 generator_loss 0.729\n",
      "Epoch 9 Iteration 600: discriminator_loss 0.672 generator_loss 0.758\n",
      "Epoch 9 Iteration 700: discriminator_loss 0.668 generator_loss 0.793\n",
      "Epoch 9 Iteration 800: discriminator_loss 0.657 generator_loss 0.711\n",
      "Epoch 9 Iteration 900: discriminator_loss 0.695 generator_loss 0.731\n",
      "Epoch 9 Iteration 938: discriminator_loss 0.676 generator_loss 0.670\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.generator'>: it's not the same object as __main__.generator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Iteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: discriminator_loss \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m generator_loss \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, idx, D_loss\u001b[38;5;241m.\u001b[39mitem(), G_loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGenerator_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/serialization.py:653\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    651\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    652\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 653\u001b[0m pickler\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m    654\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    655\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.generator'>: it's not the same object as __main__.generator"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Процедура обучения сети.\n",
    "Каждый шаг потери обновляется как для дискиминатора, так и для генератора.\n",
    "Дискриминатор стремится классифицировать реальные и fakes\n",
    "Генератор стремится генерировать как можно более реалистичные изображения\n",
    "\"\"\"\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for idx, (imgs, _) in enumerate(train_loader):\n",
    "        idx += 1\n",
    "        # Обучаем дискриминатор\n",
    "        # real_inputs - изображения из набора данных MNIST \n",
    "        # fake_inputs - изображения от генератора\n",
    "        # real_inputs должны быть классифицированы как 1, а fake_inputs - как 0\n",
    "        real_inputs = imgs.to(device)\n",
    "        real_outputs = D(real_inputs)\n",
    "        real_label = torch.ones(real_inputs.shape[0], 1).to(device)\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_label = torch.zeros(fake_inputs.shape[0], 1).to(device)\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_label), 0)\n",
    "        D_loss = loss(outputs, targets)\n",
    "        D_optimizer.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        # Обучаем генератор\n",
    "        # Цель генератора получить от дискриминатора 1 по всем изображениям\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128)-0.5)/0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "        fake_targets = torch.ones([fake_inputs.shape[0], 1]).to(device)\n",
    "        G_loss = loss(fake_outputs, fake_targets)\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        if idx % 100 == 0 or idx == len(train_loader):\n",
    "            print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'.format(epoch, idx, D_loss.item(), G_loss.item()))\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save(G, 'Generator_epoch_{}.pth'.format(epoch))\n",
    "        print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb760a73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 25088 into shape (1,28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mforward(noise)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 25088 into shape (1,28,28)"
     ]
    }
   ],
   "source": [
    "x = G.forward(noise).detach().numpy()\n",
    "x.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29fa6ab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(32, 1, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m; sns\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mВизуализация массива\u001b[39m\u001b[38;5;124m\"\u001b[39m,fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual_numpy_array.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/seaborn/matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[1;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/seaborn/matrix.py:110\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[0;32m--> 110\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[1;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    747\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    755\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:315\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    309\u001b[0m     _copy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    310\u001b[0m         copy_on_sanitize\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, dtype))\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    314\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, copy\u001b[38;5;241m=\u001b[39m_copy)\n\u001b[0;32m--> 315\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/internals/construction.py:570\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    568\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(32, 1, 28, 28)"
     ]
    }
   ],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.heatmap(x, annot=True, fmt=\"d\")\n",
    "plt.title(\"Визуализация массива\",fontsize=12)\n",
    "plt.savefig(\"visual_numpy_array.png\", bbox_inches='tight', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d471a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
